import io.delta.tables.DeltaTable;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

import java.util.HashMap;
import java.util.Map;

public class ReadDeltaTableWithBasePath {
    public static void main(String[] args) {
        // Initialize Spark session
        SparkSession spark = SparkSession.builder()
                .appName("ReadDeltaTableWithBasePath")
                .master("local") // Use "local" for local testing
                .getOrCreate();

        // Sample Delta table path with multiple partition information
        String deltaTablePath = "/path/to/delta-table/year=2023/month=01/day=15/region=US/";

        // Extract partition key-value pairs and the base path
        String basePath = extractBasePath(deltaTablePath);
        Map<String, String> partitions = extractPartitions(deltaTablePath);

        // Load the Delta table using the base path (without partition values)
        DeltaTable deltaTable = DeltaTable.forPath(spark, basePath);

        // Apply partition filters dynamically
        Dataset<Row> partitionedData = deltaTable.toDF();
        for (Map.Entry<String, String> entry : partitions.entrySet()) {
            partitionedData = partitionedData.filter(entry.getKey() + " = " + formatFilterValue(entry.getKey(), entry.getValue()));
        }

        // Show the filtered data
        partitionedData.show();

        // Stop Spark session
        spark.stop();
    }

    // Method to extract the base path (without partitions)
    public static String extractBasePath(String deltaTablePath) {
        StringBuilder basePath = new StringBuilder();
        String[] pathParts = deltaTablePath.split("/");

        for (String part : pathParts) {
            if (!part.contains("=")) {
                // Add only non-partition parts of the path
                basePath.append(part).append("/");
            } else {
                break;  // Stop when partition part is encountered
            }
        }

        return basePath.toString();
    }

    // Method to extract partition key-value pairs from the path
    public static Map<String, String> extractPartitions(String deltaTablePath) {
        Map<String, String> partitions = new HashMap<>();
        String[] pathParts = deltaTablePath.split("/");

        for (String part : pathParts) {
            if (part.contains("=")) {
                String[] partitionKeyValue = part.split("=");
                partitions.put(partitionKeyValue[0], partitionKeyValue[1]);  // Store partition key and value
            }
        }

        return partitions;
    }

    // Method to format filter values for strings (e.g., region = 'US')
    public static String formatFilterValue(String key, String value) {
        if (key.equals("region")) {  // Example for handling string-based partitions
            return "'" + value + "'";  // Add quotes around string partition values
        }
        return value;  // For numeric partition values, return as-is
    }
}

import io.delta.tables.DeltaTable;
import org.apache.spark.sql.SparkSession;
import java.nio.file.Paths;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import java.util.Arrays;

public class ConvertParquetToDeltaWithPartitionsFromPath {
    public static void main(String[] args) {
        // Initialize Spark session with Delta Lake support
        SparkSession spark = SparkSession.builder()
                .appName("ConvertParquetToDeltaWithPartitionsFromPath")
                .master("local")
                .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                .getOrCreate();

        // Path to the partitioned Parquet table
        String parquetTablePath = "/path/to/partitioned-parquet-table";

        // Step 1: Extract partition columns dynamically from the directory structure
        String partitionSchema = extractPartitionSchemaFromPath(parquetTablePath);

        // Step 2: Convert the Parquet table to a Delta table
        if (!partitionSchema.isEmpty()) {
            DeltaTable.convertToDelta(spark, "parquet.`" + parquetTablePath + "`", partitionSchema);
        } else {
            // If no partition columns, proceed without a partition schema
            DeltaTable.convertToDelta(spark, "parquet.`" + parquetTablePath + "`");
        }

        // Step 3: Verify the Delta table conversion (optional)
        DeltaTable deltaTable = DeltaTable.forPath(spark, parquetTablePath);
        deltaTable.toDF().show();  // Display the contents of the newly converted Delta table

        // Stop Spark session
        spark.stop();
    }

    // Helper method to extract partition columns from path
    private static String extractPartitionSchemaFromPath(String parquetTablePath) {
        // Example: /path/to/delta-table/year=2023/month=01/
        // Regex pattern to detect partition folders (e.g., year=2023)
        Pattern partitionPattern = Pattern.compile("([^/]+)=([^/]+)");

        // Get the partitions from the folder structure
        return Arrays.stream(Paths.get(parquetTablePath).toFile().listFiles())
                .filter(file -> file.isDirectory())
                .map(file -> {
                    String folderName = file.getName();
                    if (partitionPattern.matcher(folderName).find()) {
                        String[] parts = folderName.split("=");
                        return parts[0] + " STRING";  // Assuming all partitions are STRING type for simplicity
                    }
                    return "";
                })
                .filter(partition -> !partition.isEmpty())
                .collect(Collectors.joining(", "));
    }
}


